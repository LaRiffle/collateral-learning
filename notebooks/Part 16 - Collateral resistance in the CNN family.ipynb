{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Encryption - Classification and information leakage\n",
    "\n",
    "\n",
    "### Purpose\n",
    "\n",
    "We do the same task as part 15 except the collateral CNN attacking is bigger than the one used for the resistance\n",
    "\n",
    "As a remainder, what we want to do is to optimize the following problem:\n",
    "$ \\min_Q [ \\min_C l_C(Q, C) - \\alpha \\min_F l_F(Q, F) ] $\n",
    "\n",
    "\n",
    "Hence, the game is in 3 epoch time (and `F` is a CNN):\n",
    " - The normal phase where both tasks learn and strenghten before the joint optimisation:\n",
    "  - `Optimize: Q + C`\n",
    "  - `Optimize: Freezed(Q) + F`\n",
    " - The phase with the joint optimisation, where C and F are updated depending on the Q variations and Q is optimised to reduce the loss `C - alpha * F`:\n",
    "  - `Optimize: Freezed(Q) + C`\n",
    "  - `Optimize: Freezed(Q) + F`\n",
    "  - `Optimize : Q + [Freezed(C) - alpha * Freezed(F)]`\n",
    " - The recovery part, where both tasks recover from the perturbation, `Q` won't change now:\n",
    "  - `Optimize: Freezed(Q) + C`\n",
    "  - `Optimize: Freezed(Q) + F`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Joint optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the code directly from the repo, to make the notebook more readable. Functions are similar to those presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import learn\n",
    "from learn import load_data, show_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Parameters for the training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.sabotage_epochs = 10\n",
    "        self.recovering_epochs = 20\n",
    "        self.lr = 0.002\n",
    "        self.momentum = 0.5\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model with the describes architecture. Basically you have 3 blocs: 1 quadratic and 2 CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHARS = 26\n",
    "N_FONTS = 5\n",
    "class QuadConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadConvNet, self).__init__()\n",
    "        self.proj1 = nn.Linear(784, 50)\n",
    "        self.diag1 = nn.Linear(50, 8)\n",
    "        \n",
    "        # --- FFN for characters\n",
    "        self.lin1 = nn.Linear(8, 16)\n",
    "        self.lin2 = nn.Linear(16, N_CHARS)\n",
    "        \n",
    "        # --- Junction\n",
    "        self.jct = nn.Linear(8, 784)\n",
    "        \n",
    "        # --- CNN for families\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, N_FONTS)\n",
    "        \n",
    "        # --- Junction\n",
    "        self.adv_jct = nn.Linear(8, 784)\n",
    "        \n",
    "        # --- Advanced CNN for familiers\n",
    "        self.adv_conv1 = nn.Conv2d(1, 30, 4, 1)\n",
    "        self.adv_conv2 = nn.Conv2d(30, 100, 4)\n",
    "        self.adv_fc1 = nn.Linear(100 * 4 * 4, 1000)\n",
    "        self.adv_fc2 = nn.Linear(1000, 100)\n",
    "        self.adv_fc3 = nn.Linear(100, N_FONTS)\n",
    "        \n",
    "    def quad(self, x):\n",
    "        \"\"\"Same as forward up to the junction part\n",
    "        Used for the collateral training\"\"\"\n",
    "        # --- Quadratic \n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_char(self, x):\n",
    "        # --- FFN\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "        \n",
    "    def conv_font(self, x):\n",
    "        # --- Junction\n",
    "        x = self.jct(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # --- CNN\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_adv_font(self, x):\n",
    "        # --- Junction\n",
    "        x = self.adv_jct(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # --- CNN\n",
    "        x = F.relu(self.adv_conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.adv_conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*100)\n",
    "        x = F.relu(self.adv_fc1(x))\n",
    "        x = F.relu(self.adv_fc2(x))\n",
    "        x = self.adv_fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward_char(self, x):\n",
    "        x = self.quad(x)\n",
    "        x = self.conv_char(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def forward_font(self, x):\n",
    "        x = self.quad(x)\n",
    "        x = self.conv_font(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def forward_adv_font(self, x):\n",
    "        x = self.quad(x)\n",
    "        x = self.conv_adv_font(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def get_params(self, net):\n",
    "        \"\"\"Select the params for a given part of the net\"\"\"\n",
    "        if net == 'quad':\n",
    "            layers = [self.proj1, self.diag1]\n",
    "        elif net == 'char':\n",
    "            layers = [self.lin1, self.lin2]\n",
    "        elif net == 'font':\n",
    "            layers = [self.jct, self.fc1, self.fc2, self.conv1, self.conv2]\n",
    "        elif net == 'adv_font':\n",
    "            layers = [self.adv_jct, self.adv_fc1, self.adv_fc2, self.adv_fc3, self.adv_conv1, self.adv_conv2]\n",
    "        else:\n",
    "            raise AttributeError(f'{net} type not recognized')\n",
    "        params = [p for layer in layers for p in layer.parameters()]\n",
    "        return params\n",
    "    \n",
    "    def freeze(self, net):\n",
    "        \"\"\"Freeze a part of the net\"\"\"\n",
    "        net_params = self.get_params(net)\n",
    "        for param in net_params:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze(self):\n",
    "        \"\"\"Unfreeze the net\"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the train and test functions. They assume the train_load return two labels: the char and the font of some input.\n",
    "\n",
    "In the training phase we execute the 3 steps as described aboved.\n",
    "\n",
    "In the test function, we just test the performance for the main and collateral tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch, alpha, initial_phase, perturbate, recover):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Split the two targets\n",
    "        target_char = target[:, 0]\n",
    "        target_font = target[:, 1]\n",
    "        \n",
    "        if not recover: # building resistance\n",
    "            # Optimize C and F parts of the model with 2 strategies\n",
    "            if initial_phase:  # Normal optimisation of C\n",
    "                # Phase 1: Optimise Q + C\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward_char(data)\n",
    "                loss_char = F.nll_loss(output, target_char)\n",
    "                loss_char.backward()\n",
    "                optimizer.step()\n",
    "            else: # Freezed(Q) optimisation\n",
    "                # Phase 1: Optimise Freezed(Q) + C\n",
    "                model.freeze('quad')\n",
    "                optimizer.zero_grad()\n",
    "                output_char = model.forward_char(data)\n",
    "                loss_char = F.nll_loss(output_char, target_char)\n",
    "                loss_char.backward()\n",
    "                optimizer.step()\n",
    "                model.unfreeze()\n",
    "            # Phase 2: Optimise Freezed(Q) + F\n",
    "            model.freeze('quad')\n",
    "            output_font = model.forward_font(data)\n",
    "            loss_font = F.nll_loss(output_font, target_font)\n",
    "            loss_font.backward()\n",
    "            optimizer.step()\n",
    "            model.unfreeze()\n",
    "\n",
    "            if perturbate:\n",
    "                # Optimize Q\n",
    "                model.freeze('font')\n",
    "                model.freeze('char')\n",
    "                optimizer.zero_grad()\n",
    "                output_char = model.forward_char(data)\n",
    "                loss_char = F.nll_loss(output_char, target_char)\n",
    "\n",
    "                output_font = model.forward_font(data)\n",
    "                loss_font = F.nll_loss(output_font, target_font)\n",
    "\n",
    "                loss = loss_char - alpha * loss_font\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.unfreeze()\n",
    "        else: # recover phase: we change the collateral adversary\n",
    "            loss_char = 0.\n",
    "            model.freeze('quad')\n",
    "            optimizer.zero_grad()\n",
    "            output_font = model.forward_adv_font(data)\n",
    "            loss_font = F.nll_loss(output_font, target_font)\n",
    "            loss_font.backward()\n",
    "            optimizer.step()\n",
    "            model.unfreeze()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss_char = loss_char.item() if loss_char != 0 else loss_char\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss Char: {:.6f} Loss Font: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss_char, loss_font.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader, recover):\n",
    "    model.eval()\n",
    "    correct_char = 0\n",
    "    correct_font = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Split the two targets\n",
    "            target_char = target[:, 0]\n",
    "            target_font = target[:, 1]\n",
    "\n",
    "            # Char evaluation\n",
    "            output = model.forward_char(data)\n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct_char += pred.eq(target_char.view_as(pred)).sum().item()\n",
    "            \n",
    "            # Font evaluation\n",
    "            if not recover:\n",
    "                output = model.forward_font(data)\n",
    "            else:\n",
    "                output = model.forward_adv_font(data)\n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct_font += pred.eq(target_font.view_as(pred)).sum().item()\n",
    "\n",
    "    acc_char = 100. * correct_char / len(test_loader.dataset)\n",
    "    acc_font = 100. * correct_font / len(test_loader.dataset)\n",
    "    print('\\nTest set: Accuracy Char : {}/{} ({:.2f}%)\\n          Accuracy Font : {}/{} ({:.2f}%)'.format(\n",
    "        correct_char, len(test_loader.dataset), acc_char, correct_font, len(test_loader.dataset), acc_font))\n",
    "\n",
    "    return acc_char, acc_font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can handle the main function. The only thing that really changes is that we now want to have both labels associated to a input item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor_dataset(data, target):\n",
    "    \"\"\"Utility function to cast our data into a normalized torch TensorDataset\"\"\"\n",
    "    normed_data = [(d - d.mean()) / d.std() for d in data]\n",
    "    normed_data = torch.stack([torch.Tensor(d).reshape(1, 28, 28) for d in normed_data])\n",
    "    target = torch.LongTensor([[i[0][0], i[1][0]] for i in target])\n",
    "    tensor_dataset = utils.TensorDataset(normed_data, target)\n",
    "    return tensor_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last the main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(alpha=0):\n",
    "    \"\"\"\n",
    "    Perform a learning phase\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1)\n",
    "    args = Parser()\n",
    "\n",
    "    data = load_data()\n",
    "    train_data, train_target_char, train_target_family, test_data, test_target_char, test_target_family = data\n",
    "    # Merge the target datasets\n",
    "    train_target = list(zip(train_target_char, train_target_family))\n",
    "    test_target = list(zip(test_target_char, test_target_family))\n",
    "\n",
    "    # We use here the slightly modified version of this function\n",
    "    train_dataset = build_tensor_dataset(train_data, train_target)\n",
    "    test_dataset = build_tensor_dataset(test_data, test_target)\n",
    "\n",
    "    train_loader = utils.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = utils.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size, shuffle=True\n",
    "    )\n",
    "   \n",
    "    model = QuadConvNet()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    \n",
    "    test_perfs_char = []\n",
    "    test_perfs_font = []\n",
    "    \n",
    "    for epoch in range(1, args.epochs + args.sabotage_epochs + args.recovering_epochs + 1):\n",
    "        initial_phase = epoch <= args.epochs\n",
    "        if initial_phase:\n",
    "            print(\"(initial phase)\")\n",
    "        perturbate = epoch > args.epochs and epoch <= args.epochs + args.sabotage_epochs\n",
    "        if perturbate:\n",
    "            print(\"(perturbate)\")\n",
    "        recover = epoch > args.epochs + args.sabotage_epochs\n",
    "        if recover:\n",
    "            print(\"(recover: new adversary)\")\n",
    "        \n",
    "        assert initial_phase or perturbate or recover\n",
    "        \n",
    "        train(args, model, train_loader, optimizer, epoch, alpha, initial_phase, perturbate, recover)\n",
    "        test_perf_char, test_perf_font = test(args, model, test_loader, recover)\n",
    "        test_perfs_char.append(test_perf_char)\n",
    "        test_perfs_font.append(test_perf_font)\n",
    "\n",
    "    return test_perfs_char, test_perfs_font, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "alphas = [1.5] #0.05, 0.1, 0.2, 0.5, 1, 1.5, 2\n",
    "for alpha in alphas:\n",
    "    test_perfs_char_perturbate, test_perfs_font_perturbate, model_perturbate = main(alpha=alpha)\n",
    "    results[f\"Main task with perturbation alpha={alpha}\"] = test_perfs_char_perturbate\n",
    "    results[f\"Collateral task with perturbation alpha={alpha}\"] = test_perfs_font_perturbate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_results = {k:v for k, v in results.items() if 'Main' in k}\n",
    "show_results(main_results, \"The impact of perturbation with recovering phase on the main task\", ymin=50, ymax=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collateral_results = {k:v for k, v in results.items() if 'Collateral' in k}\n",
    "show_results(collateral_results, \"The impact of perturbation with recovering phase on the collateral task\", ymin=26, ymax=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to Part 15, we observe that the large model does not behave much better at all, which suggest we have scaled resistance at a sufficient point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "So the resistance in the CNN family is also pretty good as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
