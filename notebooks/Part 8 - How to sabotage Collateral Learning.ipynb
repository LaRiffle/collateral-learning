{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Encryption - Classification and information leakage\n",
    "\n",
    "Our start point is the work on encrypted classification using Function Encryption of the paper [Reading in the Dark: Classifying Encrypted Digits with Functional Encryption](https://eprint.iacr.org/2018/206), and the associated [GitHub repository](https://github.com/edufoursans/reading-in-the-dark).\n",
    "\n",
    "More specifically, the paper provides a new Functional Encryption scheme for quadratic multi-variate polynomials, which can under some hypothesis be seen as a single hidden layer neural network with a quadratic activation.\n",
    "In the paper, the output corresponds to element per class, and it is made in clear. We analyse how this output can disclose information about the initial input or about charasteristics of this input.\n",
    "\n",
    "To this aim, we have just built a dataset which is very similar to MNIST, used in the original paper but which is composed of 26 letter characters of 5 differents fonts. Our goal is two-fold:\n",
    " - Evaluate how the output in clear can be leverage with a public NN to make better prediction than a simple `argmax` function in the character recognition task.\n",
    " - Analyse to what extent the output in clear of the model trained for character recognition can reveal information about the font used, using an \"adversarial\" network.\n",
    " \n",
    " \n",
    "### Purpose\n",
    "\n",
    "We have shown that reducing the number of outputs to 8 was a good choice but wasn't enough to prevent information leakage in a general context, as this output could still be used to identify font families quite precisely. We will now try to undermine specifically the font family recognition task.\n",
    "\n",
    "In this setting, we call `Q` the quadratic part, `Cc` the CNN on top of `Q` used to predict characters and `Cf` the CNN for predicting families. The network will be structured as follows:\n",
    "````\n",
    "  |\n",
    "  Q\n",
    " / \\\n",
    "Cf  Cc\n",
    "|   |\n",
    "```\n",
    "\n",
    "We will do a 3-phase process:\n",
    " - `Optimize: Q + Cc`\n",
    " - `Optimize: Freezed(Q) + Cf`\n",
    " - `Perturb : Q + Freezed(Cf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN = False  # Should you retrain models that you already trained and stored.\n",
    "# Set it to True if you change of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Main task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the code directly from the repo, to make the notebook more readable. Functions are similar to those presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import learn\n",
    "from learn import load_data, show_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Parameters for the training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.002\n",
    "        self.momentum = 0.5\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model with the describes architecture. Basically you have 3 blocs: 1 quadratic and 2 CNN. Also we have defined a freeze / unfreeze functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHARS = 26\n",
    "N_FONTS = 5\n",
    "class QuadConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadConvNet, self).__init__()\n",
    "        self.proj1 = nn.Linear(784, 50)\n",
    "        self.diag1 = nn.Linear(50, 8)\n",
    "        \n",
    "        # --- Junction\n",
    "        self.jct = nn.Linear(8, 784)\n",
    "        \n",
    "        # --- CNN for characters\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, N_CHARS)\n",
    "        \n",
    "        # --- CNN for font families\n",
    "        self.conv3 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv4 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc3 = nn.Linear(4*4*50, 500)\n",
    "        self.fc4 = nn.Linear(500, N_FONTS)\n",
    "        \n",
    "    def quad(self, x):\n",
    "        \"\"\"Same as forward up to the junction part\n",
    "        Used for the collateral training\"\"\"\n",
    "        # --- Quadratic \n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return x\n",
    "    \n",
    "    def conv_char(self, x):\n",
    "        # --- Junction\n",
    "        x = self.jct(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # --- CNN\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "    def conv_font(self, x):\n",
    "        # --- Junction\n",
    "        x = self.jct(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        # --- CNN\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def forward_char(self, x):\n",
    "        x = self.quad(x)\n",
    "        x = self.conv_char(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def forward_font(self, x):\n",
    "        x = self.quad(x)\n",
    "        x = self.conv_font(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def get_params(self, net):\n",
    "        \"\"\"Select the params for a given part of the net\"\"\"\n",
    "        if net == 'quad':\n",
    "            params = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
    "        elif net == 'font':\n",
    "            params = [self.conv3, self.conv4, self.fc3, self.fc4]\n",
    "        else:\n",
    "            raise AttributeError(f'{net} type not recognized')\n",
    "        return params\n",
    "    \n",
    "    def freeze(self, net):\n",
    "        \"\"\"Freeze a part of the net\"\"\"\n",
    "        net_params = self.get_params(net)\n",
    "        for param in net_params:\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def unfreeze(self):\n",
    "        \"\"\"Unfreeze the net\"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the train and test functions. They assume the train_load return two labels: the char and the font of some input. In the training phase we execute the 3 stesp as described aboved. In the test function, we juste test the performance for the main and collateral tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Split the two targets\n",
    "        target_char = target[:, 0]\n",
    "        target_font = target[:, 1]\n",
    "        \n",
    "        # Phase 1: Optimise Q + Cc\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward_char(data)\n",
    "        loss_char = F.nll_loss(output, target_char)\n",
    "        loss_char.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Phase 2: Optimise Freezed(Q) + Cf\n",
    "        model.freeze('quad')\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward_font(data)\n",
    "        loss_font = F.nll_loss(output, target_font)\n",
    "        loss_font.backward()\n",
    "        optimizer.step()\n",
    "        model.unfreeze()\n",
    "        \n",
    "        # Phase 3: Perturb Q + Freezed(Cf)\n",
    "        model.freeze('font')\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward_font(data)\n",
    "        fake_target = torch.tensor(np.random.randint(0, N_FONTS, len(data)))\n",
    "        fake_loss = F.nll_loss(output, fake_target)\n",
    "        fake_loss.backward()\n",
    "        optimizer.step()\n",
    "        model.unfreeze()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss Char: {:.6f} Loss Font: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss_char.item(), loss_font.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    #test_loss = 0\n",
    "    correct_char = 0\n",
    "    correct_font = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Split the two targets\n",
    "            target_char = target[:, 0]\n",
    "            target_font = target[:, 1]\n",
    "\n",
    "            # Char evaluation\n",
    "            output = model.forward_char(data)\n",
    "            #test_loss += F.nll_loss(output, target_char, reduction='sum').item()  \n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct_char += pred.eq(target_char.view_as(pred)).sum().item()\n",
    "            \n",
    "            # Font evaluation\n",
    "            output = model.forward_font(data)\n",
    "            #test_loss += F.nll_loss(output, target_font, reduction='sum').item()  \n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct_font += pred.eq(target_font.view_as(pred)).sum().item()\n",
    "\n",
    "    #test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    acc_char = 100. * correct_char / len(test_loader.dataset)\n",
    "    acc_font = 100. * correct_font / len(test_loader.dataset)\n",
    "    print('\\nTest set: Accuracy Char : {}/{} ({:.2f}%)\\n          Accuracy Font : {}/{} ({:.2f}%)'.format(\n",
    "        correct_char, len(test_loader.dataset), acc_char, correct_font, len(test_loader.dataset), acc_font))\n",
    "\n",
    "    return acc_char, acc_font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can handle the main function. The only thing that really changes is that we now want to have both labels associated to a input item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tensor_dataset(data, target):\n",
    "    \"\"\"Utility function to cast our data into a normalized torch TensorDataset\"\"\"\n",
    "    normed_data = [(d - d.mean()) / d.std() for d in data]\n",
    "    normed_data = torch.stack([torch.Tensor(d).reshape(1, 28, 28) for d in normed_data])\n",
    "    target = torch.LongTensor([[i[0][0], i[1][0]] for i in target])\n",
    "    tensor_dataset = utils.TensorDataset(normed_data, target)\n",
    "    return tensor_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last the main function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Perform a learning phase\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1)\n",
    "    args = Parser()\n",
    "\n",
    "    data = load_data()\n",
    "    train_data, train_target_char, train_target_family, test_data, test_target_char, test_target_family = data\n",
    "    # Merge the target datasets\n",
    "    train_target = list(zip(train_target_char, train_target_family))\n",
    "    test_target = list(zip(test_target_char, test_target_family))\n",
    "\n",
    "    # We use here the slightly modified version of this function\n",
    "    train_dataset = build_tensor_dataset(train_data, train_target)\n",
    "    test_dataset = build_tensor_dataset(test_data, test_target)\n",
    "\n",
    "    train_loader = utils.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = utils.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size, shuffle=True\n",
    "    )\n",
    "   \n",
    "    model = QuadConvNet()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    test_perfs_char = []\n",
    "    test_perfs_font = []\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, train_loader, optimizer, epoch)\n",
    "        test_perf_char, test_perf_font = test(args, model, test_loader)\n",
    "        test_perfs_char.append(test_perf_char)\n",
    "        test_perfs_font.append(test_perf_font)\n",
    "\n",
    "    return test_perfs_char, test_perfs_font, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set 60000 items\n",
      "Testing set  10000 items\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss Char: 3.267608 Loss Font: 1.610325\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss Char: 3.252979 Loss Font: 1.607863\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss Char: 3.261102 Loss Font: 1.615057\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss Char: 3.254089 Loss Font: 1.607399\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss Char: 3.266242 Loss Font: 1.604725\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss Char: 3.250765 Loss Font: 1.611383\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss Char: 3.256726 Loss Font: 1.608303\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss Char: 3.252547 Loss Font: 1.605977\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss Char: 3.236609 Loss Font: 1.610075\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss Char: 3.245800 Loss Font: 1.600885\n",
      "\n",
      "Test set: Accuracy Char : 558/10000 (5.58%)\n",
      "          Accuracy Font : 2029/10000 (20.29%)\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss Char: 3.251272 Loss Font: 1.602674\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss Char: 3.237260 Loss Font: 1.595951\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss Char: 3.185588 Loss Font: 1.588099\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss Char: 3.159889 Loss Font: 1.598905\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss Char: 2.983756 Loss Font: 1.577134\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss Char: 2.624593 Loss Font: 1.598918\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss Char: 2.353958 Loss Font: 1.570463\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss Char: 2.158056 Loss Font: 1.553018\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss Char: 1.857651 Loss Font: 1.573719\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss Char: 2.030418 Loss Font: 1.579038\n",
      "\n",
      "Test set: Accuracy Char : 3234/10000 (32.34%)\n",
      "          Accuracy Font : 2983/10000 (29.83%)\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss Char: 1.868882 Loss Font: 1.530367\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss Char: 1.774502 Loss Font: 1.555337\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss Char: 1.800455 Loss Font: 1.552830\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss Char: 1.744604 Loss Font: 1.573804\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss Char: 1.587835 Loss Font: 1.507887\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss Char: 1.519840 Loss Font: 1.588119\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss Char: 1.397695 Loss Font: 1.477912\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss Char: 1.408972 Loss Font: 1.495837\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss Char: 1.495113 Loss Font: 1.464565\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss Char: 1.111809 Loss Font: 1.555067\n",
      "\n",
      "Test set: Accuracy Char : 5768/10000 (57.68%)\n",
      "          Accuracy Font : 3514/10000 (35.14%)\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss Char: 1.252671 Loss Font: 1.473430\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss Char: 1.137586 Loss Font: 1.510851\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss Char: 1.440449 Loss Font: 1.438989\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss Char: 1.193982 Loss Font: 1.547611\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss Char: 1.044275 Loss Font: 1.455947\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss Char: 1.013105 Loss Font: 1.462149\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss Char: 1.022843 Loss Font: 1.459565\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss Char: 0.683369 Loss Font: 1.422539\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss Char: 0.947926 Loss Font: 1.431133\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss Char: 0.809180 Loss Font: 1.450074\n",
      "\n",
      "Test set: Accuracy Char : 7373/10000 (73.73%)\n",
      "          Accuracy Font : 4353/10000 (43.53%)\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss Char: 0.976822 Loss Font: 1.424981\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss Char: 0.848577 Loss Font: 1.385719\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss Char: 0.748919 Loss Font: 1.395852\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss Char: 0.658626 Loss Font: 1.364290\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss Char: 0.495504 Loss Font: 1.346958\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss Char: 0.432495 Loss Font: 1.405157\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss Char: 0.532723 Loss Font: 1.390189\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss Char: 0.685449 Loss Font: 1.307781\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss Char: 0.658083 Loss Font: 1.408015\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss Char: 0.753087 Loss Font: 1.354354\n",
      "\n",
      "Test set: Accuracy Char : 7729/10000 (77.29%)\n",
      "          Accuracy Font : 4542/10000 (45.42%)\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss Char: 0.473889 Loss Font: 1.326462\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss Char: 0.377604 Loss Font: 1.259752\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss Char: 0.417093 Loss Font: 1.248006\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss Char: 0.413657 Loss Font: 1.343314\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss Char: 0.520228 Loss Font: 1.302199\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss Char: 0.359554 Loss Font: 1.348971\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss Char: 0.291889 Loss Font: 1.275252\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss Char: 0.363165 Loss Font: 1.330031\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss Char: 0.385610 Loss Font: 1.257079\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss Char: 0.475526 Loss Font: 1.245884\n",
      "\n",
      "Test set: Accuracy Char : 8603/10000 (86.03%)\n",
      "          Accuracy Font : 5216/10000 (52.16%)\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss Char: 0.306808 Loss Font: 1.221630\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss Char: 0.412358 Loss Font: 1.249391\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss Char: 1.008115 Loss Font: 1.236873\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss Char: 0.480427 Loss Font: 1.224551\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss Char: 0.214770 Loss Font: 1.249263\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss Char: 0.299378 Loss Font: 1.265789\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss Char: 0.469258 Loss Font: 1.306202\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss Char: 0.381063 Loss Font: 1.180292\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss Char: 0.253340 Loss Font: 1.198302\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss Char: 0.288632 Loss Font: 1.200260\n",
      "\n",
      "Test set: Accuracy Char : 8558/10000 (85.58%)\n",
      "          Accuracy Font : 5701/10000 (57.01%)\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss Char: 0.427291 Loss Font: 1.207668\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss Char: 0.362409 Loss Font: 1.212790\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss Char: 0.352174 Loss Font: 1.173947\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss Char: 0.244596 Loss Font: 1.174399\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss Char: 0.275367 Loss Font: 1.133134\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss Char: 0.378256 Loss Font: 1.219265\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss Char: 0.399096 Loss Font: 1.150521\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss Char: 0.371744 Loss Font: 1.160457\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss Char: 0.221821 Loss Font: 1.203195\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss Char: 0.345554 Loss Font: 1.264312\n",
      "\n",
      "Test set: Accuracy Char : 9001/10000 (90.01%)\n",
      "          Accuracy Font : 6077/10000 (60.77%)\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss Char: 0.380956 Loss Font: 1.284415\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss Char: 0.169362 Loss Font: 1.138680\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss Char: 0.481280 Loss Font: 1.319568\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss Char: 0.356470 Loss Font: 1.140587\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss Char: 0.158109 Loss Font: 1.170905\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss Char: 0.312163 Loss Font: 1.128537\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss Char: 0.168584 Loss Font: 1.160691\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss Char: 0.133706 Loss Font: 1.106130\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss Char: 0.223266 Loss Font: 1.194431\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss Char: 0.232637 Loss Font: 1.105483\n",
      "\n",
      "Test set: Accuracy Char : 9005/10000 (90.05%)\n",
      "          Accuracy Font : 6265/10000 (62.65%)\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss Char: 0.230778 Loss Font: 1.174565\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss Char: 0.600713 Loss Font: 1.174773\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss Char: 0.209269 Loss Font: 1.122066\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss Char: 0.162504 Loss Font: 1.092587\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss Char: 0.195682 Loss Font: 1.130702\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss Char: 0.210554 Loss Font: 1.058290\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss Char: 0.269451 Loss Font: 1.009504\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss Char: 0.279846 Loss Font: 1.104461\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss Char: 0.133895 Loss Font: 1.066258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss Char: 0.218759 Loss Font: 1.048191\n",
      "\n",
      "Test set: Accuracy Char : 9160/10000 (91.60%)\n",
      "          Accuracy Font : 6339/10000 (63.39%)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_perfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-7aad8d425c69>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtest_perfs_font\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_perf_font\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_perfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_perfs' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the results that we get don't help to favour the main task against the second one, as the drop in performance is approximately the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
