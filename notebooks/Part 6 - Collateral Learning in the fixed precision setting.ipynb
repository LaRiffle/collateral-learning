{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Encryption - Classification and information leakage\n",
    "\n",
    "Our start point is the work on encrypted classification using Function Encryption of the paper [Reading in the Dark: Classifying Encrypted Digits with Functional Encryption](https://eprint.iacr.org/2018/206), and the associated [GitHub repository](https://github.com/edufoursans/reading-in-the-dark).\n",
    "\n",
    "More specifically, the paper provides a new Functional Encryption scheme for quadratic multi-variate polynomials, which can under some hypothesis be seen as a single hidden layer neural network with a quadratic activation.\n",
    "In the paper, the output corresponds to element per class, and it is made in clear. We analyse how this output can disclose information about the initial input or about charasteristics of this input.\n",
    "\n",
    "To this aim, we have just built a dataset which is very similar to MNIST, used in the original paper but which is composed of 26 letter characters of 5 differents fonts. Our goal is two-fold:\n",
    " - Evaluate how the output in clear can be leverage with a public NN to make better prediction than a simple `argmax` function in the character recognition task.\n",
    " - Analyse to what extent the output in clear of the model trained for character recognition can reveal information about the font used, using a \"collateral\" network.\n",
    " \n",
    " \n",
    "### Purpose\n",
    "\n",
    "Using PySyft, we have evaluated the model in a fixed precision setting. We'll now try to assess how fixed precision can influence the collateral learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collateral Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the code directly from the repo, to make the notebook more readable. Functions are similar to those presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import learn\n",
    "from learn import main, train, test, show_results, show_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the quadratic baseline\n",
    "Let's train the baseline model and this how we can use its output to train a collateral network that tries to perform another distinct task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(QuadNet, self).__init__()\n",
    "        self.proj1 = nn.Linear(784, 50)\n",
    "        self.diag1 = nn.Linear(50, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def transform(self, x):\n",
    "        \"\"\"Same as forward but without the log_softmax\"\"\"\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the quadratic model that we saved in Part 4! _Be sure that the path and file name match._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadNet(\n",
       "  (proj1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (diag1): Linear(in_features=50, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREC_FRAC = 3\n",
    "quad_model = QuadNet(26)\n",
    "path = '../data/models/quad_char.pt'\n",
    "quad_model.load_state_dict(torch.load(path))\n",
    "quad_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the model in fixed precision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuadNet(\n",
       "  (proj1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (diag1): Linear(in_features=50, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "quad_model.fix_precision(precision_fractional=PREC_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the collateral task\n",
    "\n",
    "We will now use the output of the trained baseline model which is freezed as an input of another model called the `collateral_model` which will try to predict on another task, namely the family recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are little change compared to the usual test, train and main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collateral_train(args, collateral_model, model, train_loader, adv_optimizer, epoch, prec_frac):\n",
    "    collateral_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data.fix_precision_(precision_fractional=prec_frac)  # <-- This is new\n",
    "        data = model.transform(data)\n",
    "        data = data.float_precision() # <-- This is new\n",
    "        adv_optimizer.zero_grad()\n",
    "        output = collateral_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        adv_optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def collateral_test(args, collateral_model, model, test_loader, prec_frac):\n",
    "    collateral_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_labels = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data.fix_precision_(precision_fractional=prec_frac)  # <-- This is new\n",
    "            data = model.transform(data) # <-- This is new\n",
    "            data = data.float_precision() # <-- This is new\n",
    "            output = collateral_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), acc))\n",
    "    \n",
    "    return acc, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the collateral model that we will use. We use the same CNN structure as seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateralCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(CollateralCNN, self).__init__()\n",
    "        self.lin1 = nn.Linear(26, 784)\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.seed = 1\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.no_cuda = False\n",
    "        self.save_model = False\n",
    "        self.log_interval = 300\n",
    "        \n",
    "def build_tensor_dataset(data, target):\n",
    "    normed_data = [(d - d.mean()) / d.std() for d in data]\n",
    "    normed_data = torch.stack([torch.Tensor(d).reshape(1, 28, 28) for d in normed_data])\n",
    "    target = torch.LongTensor([i[0] for i in target])\n",
    "    tensor_dataset = utils.TensorDataset(normed_data, target)\n",
    "    return tensor_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base quadratic model is already trained to detect char, which it does not too badly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We new try to detect using its output, the family of the original input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set 60000 items\n",
      "Testing set  10000 items\n"
     ]
    }
   ],
   "source": [
    "data = learn.load_data()\n",
    "train_data, train_target_char, train_target_family, test_data, test_target_char, test_target_family = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collateral_phase(model, prec_frac):\n",
    "    args = Parser()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # setting = the family recognition task\n",
    "    train_dataset = build_tensor_dataset(train_data, train_target_family)\n",
    "    test_dataset = build_tensor_dataset(test_data, test_target_family)\n",
    "    collateral_output_size = 5\n",
    "    \n",
    "    train_loader = utils.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    test_loader = utils.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    collateral_model = CollateralCNN(output_size=collateral_output_size)\n",
    "    collateral_optimizer = optim.SGD(collateral_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    \n",
    "    test_perfs = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        collateral_train(args, collateral_model, model, train_loader, collateral_optimizer, epoch, prec_frac)\n",
    "        acc, pred_labels = collateral_test(args, collateral_model, model, test_loader, prec_frac)\n",
    "        test_perfs.append(acc)\n",
    "        \n",
    "    return test_perfs, pred_labels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryffel/Documents/Code/PySyft/syft/frameworks/torch/tensors/interpreters/native.py:194: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  response = eval(cmd)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.619203\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.964526\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.975482\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.713356\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 6843/10000 (68.43%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.761478\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.733107\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.669797\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.485431\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 7160/10000 (71.60%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.597299\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.633971\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.638734\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.583066\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 7433/10000 (74.33%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.664523\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.762269\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.536755\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.496872\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 7957/10000 (79.57%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.495304\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.439458\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.541035\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.421324\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 7890/10000 (78.90%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.285996\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.426274\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.549057\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.399147\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 7999/10000 (79.99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.454949\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.352572\n"
     ]
    }
   ],
   "source": [
    "test_perfs, pred_labels = collateral_phase(quad_model, PREC_FRAC)\n",
    "test_perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that actually the collateral learning phase is also doing very well. This shows that it doesn't rely on very small signals, and that this is a threat that we can observe in the real FE setting.\n",
    "\n",
    "Just for curiosity, we compare the learning curve with the collateral learning phase using floats. _You should report the result updated from Part 4_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cl_perf =    [69.16, 72.03, 77.94, 79.31, 78.83, 81.78, 82.19, 83.45, 83.57, 83.55]\n",
    "fp_cl_perf = [73.52, 74.98, 77.42, 80.51, 80.03, 80.55, 83.48, 83.74, 83.62, 84.38]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(cl_perf) + 1), cl_perf, label='Reference Collateral Learning accuracy')\n",
    "plt.plot(range(1, len(fp_cl_perf) + 1), fp_cl_perf, label='Fixed Prec. Collateral Learning accuracy')\n",
    "plt.ylabel('Accuracy in %')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Accuracy as a function of epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Now that we have established the threats associated with intermediate public neural network outputs, we'll try to find method to mitigate collateral information leakage without downgrading too much the accuracy on the main task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
