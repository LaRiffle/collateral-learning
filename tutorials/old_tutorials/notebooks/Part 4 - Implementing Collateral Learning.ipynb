{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Encryption - Classification and information leakage\n",
    "\n",
    "Our start point is the work on encrypted classification using Function Encryption of the paper [Reading in the Dark: Classifying Encrypted Digits with Functional Encryption](https://eprint.iacr.org/2018/206), and the associated [GitHub repository](https://github.com/edufoursans/reading-in-the-dark).\n",
    "\n",
    "More specifically, the paper provides a new Functional Encryption scheme for quadratic multi-variate polynomials, which can under some hypothesis be seen as a single hidden layer neural network with a quadratic activation.\n",
    "In the paper, the output corresponds to element per class, and it is made in clear. We analyse how this output can disclose information about the initial input or about charasteristics of this input.\n",
    "\n",
    "To this aim, we have just built a dataset which is very similar to MNIST, used in the original paper but which is composed of 26 letter characters of 5 differents fonts. Our goal is two-fold:\n",
    " - Evaluate how the output in clear can be leverage with a public NN to make better prediction than a simple `argmax` function in the character recognition task.\n",
    " - Analyse to what extent the output in clear of the model trained for character recognition can reveal information about the font used, using an \"adversarial\" network.\n",
    " \n",
    "We have used the output and modified the output of our quadratic net to improve the character classification accuracy. But can we use this now this output of the private model that we freeze, in order to learn to perform another _adversarial_ classification task, where we want instead to detect the font of the letter used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Collateral Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the code directly from the repo, to make the notebook more readable. Functions are similar to those presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import learn\n",
    "from learn import main, train, test, show_results, show_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading the quadratic baseline\n",
    "Let's train the baseline model and this how we can use its output to train a \"adversarial/collateral\" network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.log_interval = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(QuadNet, self).__init__()\n",
    "        self.proj1 = nn.Linear(784, 40)\n",
    "        self.diag1 = nn.Linear(40, output_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def transform(self, x):\n",
    "        \"\"\"Same as forward but without the log_softmax\"\"\"\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.proj1(x)\n",
    "        x = x * x\n",
    "        x = self.diag1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set 60000 items\n",
      "Testing set  10000 items\n",
      "Learning on char with quad \n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.475983\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.245228\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.766859\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.098686\n",
      "\n",
      "Test set: Average loss: 0.5489, Accuracy: 8470/10000 (84.70%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.887122\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.779222\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.619985\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.555708\n",
      "\n",
      "Test set: Average loss: 0.3844, Accuracy: 8895/10000 (88.95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.827368\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.444816\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.418822\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.363198\n",
      "\n",
      "Test set: Average loss: 0.3191, Accuracy: 9116/10000 (91.16%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.505072\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.550491\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.577900\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.527792\n",
      "\n",
      "Test set: Average loss: 0.2788, Accuracy: 9260/10000 (92.60%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.462027\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.572899\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.556444\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.393626\n",
      "\n",
      "Test set: Average loss: 0.2820, Accuracy: 9205/10000 (92.05%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.500180\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.355089\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.464469\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.456374\n",
      "\n",
      "Test set: Average loss: 0.2404, Accuracy: 9357/10000 (93.57%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.525615\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.344890\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.447878\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.456265\n",
      "\n",
      "Test set: Average loss: 0.2427, Accuracy: 9342/10000 (93.42%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.412237\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.444202\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.454197\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.375470\n",
      "\n",
      "Test set: Average loss: 0.2228, Accuracy: 9391/10000 (93.91%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.481925\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.413192\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.572874\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.422920\n",
      "\n",
      "Test set: Average loss: 0.2281, Accuracy: 9379/10000 (93.79%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.409351\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.527414\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.424007\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.360844\n",
      "\n",
      "Test set: Average loss: 0.2272, Accuracy: 9365/10000 (93.65%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.485007\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.401020\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.623331\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.355704\n",
      "\n",
      "Test set: Average loss: 0.2226, Accuracy: 9413/10000 (94.13%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.434980\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.423587\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.433972\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.408802\n",
      "\n",
      "Test set: Average loss: 0.2131, Accuracy: 9429/10000 (94.29%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.519898\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.365325\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.419751\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.419246\n",
      "\n",
      "Test set: Average loss: 0.2014, Accuracy: 9487/10000 (94.87%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.459038\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.436636\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.697212\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.365681\n",
      "\n",
      "Test set: Average loss: 0.2263, Accuracy: 9361/10000 (93.61%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.372940\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.445680\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.433660\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.522689\n",
      "\n",
      "Test set: Average loss: 0.2195, Accuracy: 9404/10000 (94.04%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.337670\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.352424\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.371395\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.313803\n",
      "\n",
      "Test set: Average loss: 0.2055, Accuracy: 9444/10000 (94.44%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.402636\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.335972\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.357867\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.463065\n",
      "\n",
      "Test set: Average loss: 0.2051, Accuracy: 9461/10000 (94.61%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.365343\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.592302\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.561522\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.396820\n",
      "\n",
      "Test set: Average loss: 0.2195, Accuracy: 9409/10000 (94.09%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.409880\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.419138\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.421690\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.475760\n",
      "\n",
      "Test set: Average loss: 0.1988, Accuracy: 9485/10000 (94.85%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.411146\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.464081\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.453811\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.364944\n",
      "\n",
      "Test set: Average loss: 0.1927, Accuracy: 9475/10000 (94.75%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.464665\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.439037\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.437863\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.354407\n",
      "\n",
      "Test set: Average loss: 0.2152, Accuracy: 9421/10000 (94.21%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.408243\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.405463\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.395428\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.387006\n",
      "\n",
      "Test set: Average loss: 0.1977, Accuracy: 9476/10000 (94.76%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.356736\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.322278\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.456373\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.400250\n",
      "\n",
      "Test set: Average loss: 0.1942, Accuracy: 9501/10000 (95.01%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.416172\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.399960\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.404222\n",
      "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 0.413758\n",
      "\n",
      "Test set: Average loss: 0.1971, Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.308165\n",
      "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 0.401970\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 0.475360\n",
      "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 0.487231\n",
      "\n",
      "Test set: Average loss: 0.2008, Accuracy: 9430/10000 (94.30%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.375698\n",
      "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 0.476327\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 0.408228\n",
      "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 0.471699\n",
      "\n",
      "Test set: Average loss: 0.1943, Accuracy: 9495/10000 (94.95%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.315680\n",
      "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 0.497049\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 0.323850\n",
      "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 0.341540\n",
      "\n",
      "Test set: Average loss: 0.2122, Accuracy: 9408/10000 (94.08%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.316361\n",
      "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 0.314536\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 0.381350\n",
      "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 0.470577\n",
      "\n",
      "Test set: Average loss: 0.1884, Accuracy: 9493/10000 (94.93%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.328113\n",
      "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 0.426869\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 0.433315\n",
      "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 0.517563\n",
      "\n",
      "Test set: Average loss: 0.1942, Accuracy: 9477/10000 (94.77%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.399180\n",
      "Train Epoch: 30 [19200/60000 (32%)]\tLoss: 0.352327\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 0.327523\n",
      "Train Epoch: 30 [57600/60000 (96%)]\tLoss: 0.426962\n",
      "\n",
      "Test set: Average loss: 0.1947, Accuracy: 9491/10000 (94.91%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.468736\n",
      "Train Epoch: 31 [19200/60000 (32%)]\tLoss: 0.365855\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 0.378831\n",
      "Train Epoch: 31 [57600/60000 (96%)]\tLoss: 0.309666\n",
      "\n",
      "Test set: Average loss: 0.2169, Accuracy: 9398/10000 (93.98%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.372129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [19200/60000 (32%)]\tLoss: 0.468779\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 0.499894\n",
      "Train Epoch: 32 [57600/60000 (96%)]\tLoss: 0.360279\n",
      "\n",
      "Test set: Average loss: 0.2055, Accuracy: 9443/10000 (94.43%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.405240\n",
      "Train Epoch: 33 [19200/60000 (32%)]\tLoss: 0.359477\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 0.339128\n",
      "Train Epoch: 33 [57600/60000 (96%)]\tLoss: 0.391066\n",
      "\n",
      "Test set: Average loss: 0.1881, Accuracy: 9510/10000 (95.10%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.292737\n",
      "Train Epoch: 34 [19200/60000 (32%)]\tLoss: 0.395397\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 0.410125\n",
      "Train Epoch: 34 [57600/60000 (96%)]\tLoss: 0.417848\n",
      "\n",
      "Test set: Average loss: 0.1891, Accuracy: 9518/10000 (95.18%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.353831\n",
      "Train Epoch: 35 [19200/60000 (32%)]\tLoss: 0.389108\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 0.394082\n",
      "Train Epoch: 35 [57600/60000 (96%)]\tLoss: 0.404935\n",
      "\n",
      "Test set: Average loss: 0.1963, Accuracy: 9466/10000 (94.66%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.362429\n",
      "Train Epoch: 36 [19200/60000 (32%)]\tLoss: 0.467302\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 0.428189\n",
      "Train Epoch: 36 [57600/60000 (96%)]\tLoss: 0.351686\n",
      "\n",
      "Test set: Average loss: 0.2163, Accuracy: 9426/10000 (94.26%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.326931\n",
      "Train Epoch: 37 [19200/60000 (32%)]\tLoss: 0.360041\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 0.553812\n",
      "Train Epoch: 37 [57600/60000 (96%)]\tLoss: 0.519135\n",
      "\n",
      "Test set: Average loss: 0.2031, Accuracy: 9468/10000 (94.68%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.379884\n",
      "Train Epoch: 38 [19200/60000 (32%)]\tLoss: 0.434681\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 0.324084\n",
      "Train Epoch: 38 [57600/60000 (96%)]\tLoss: 0.379589\n",
      "\n",
      "Test set: Average loss: 0.1898, Accuracy: 9490/10000 (94.90%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.369837\n",
      "Train Epoch: 39 [19200/60000 (32%)]\tLoss: 0.402868\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 0.361725\n",
      "Train Epoch: 39 [57600/60000 (96%)]\tLoss: 0.555062\n",
      "\n",
      "Test set: Average loss: 0.1945, Accuracy: 9473/10000 (94.73%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.401674\n",
      "Train Epoch: 40 [19200/60000 (32%)]\tLoss: 0.459653\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 0.410573\n",
      "Train Epoch: 40 [57600/60000 (96%)]\tLoss: 0.499157\n",
      "\n",
      "Test set: Average loss: 0.1907, Accuracy: 9499/10000 (94.99%)\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.430921\n",
      "Train Epoch: 41 [19200/60000 (32%)]\tLoss: 0.359321\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 0.755285\n",
      "Train Epoch: 41 [57600/60000 (96%)]\tLoss: 0.554437\n",
      "\n",
      "Test set: Average loss: 0.1994, Accuracy: 9469/10000 (94.69%)\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.495501\n",
      "Train Epoch: 42 [19200/60000 (32%)]\tLoss: 0.419486\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 0.313081\n",
      "Train Epoch: 42 [57600/60000 (96%)]\tLoss: 0.475260\n",
      "\n",
      "Test set: Average loss: 0.1889, Accuracy: 9502/10000 (95.02%)\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.307425\n",
      "Train Epoch: 43 [19200/60000 (32%)]\tLoss: 0.305609\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 0.344885\n",
      "Train Epoch: 43 [57600/60000 (96%)]\tLoss: 0.492156\n",
      "\n",
      "Test set: Average loss: 0.1844, Accuracy: 9532/10000 (95.32%)\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.341193\n",
      "Train Epoch: 44 [19200/60000 (32%)]\tLoss: 0.435337\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 0.470722\n",
      "Train Epoch: 44 [57600/60000 (96%)]\tLoss: 0.494977\n",
      "\n",
      "Test set: Average loss: 0.1789, Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.409427\n",
      "Train Epoch: 45 [19200/60000 (32%)]\tLoss: 0.364022\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 0.330831\n",
      "Train Epoch: 45 [57600/60000 (96%)]\tLoss: 0.366572\n",
      "\n",
      "Test set: Average loss: 0.1977, Accuracy: 9516/10000 (95.16%)\n",
      "\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.501384\n",
      "Train Epoch: 46 [19200/60000 (32%)]\tLoss: 0.451913\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 0.427061\n",
      "Train Epoch: 46 [57600/60000 (96%)]\tLoss: 0.444211\n",
      "\n",
      "Test set: Average loss: 0.1767, Accuracy: 9550/10000 (95.50%)\n",
      "\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.334737\n",
      "Train Epoch: 47 [19200/60000 (32%)]\tLoss: 0.408930\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 0.407206\n",
      "Train Epoch: 47 [57600/60000 (96%)]\tLoss: 0.365960\n",
      "\n",
      "Test set: Average loss: 0.1899, Accuracy: 9492/10000 (94.92%)\n",
      "\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.413042\n",
      "Train Epoch: 48 [19200/60000 (32%)]\tLoss: 0.366776\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 0.368172\n",
      "Train Epoch: 48 [57600/60000 (96%)]\tLoss: 0.481916\n",
      "\n",
      "Test set: Average loss: 0.1841, Accuracy: 9531/10000 (95.31%)\n",
      "\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.336132\n",
      "Train Epoch: 49 [19200/60000 (32%)]\tLoss: 0.299175\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 0.481506\n",
      "Train Epoch: 49 [57600/60000 (96%)]\tLoss: 0.389338\n",
      "\n",
      "Test set: Average loss: 0.1908, Accuracy: 9515/10000 (95.15%)\n",
      "\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 0.505298\n",
      "Train Epoch: 50 [19200/60000 (32%)]\tLoss: 0.363448\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 0.419478\n",
      "Train Epoch: 50 [57600/60000 (96%)]\tLoss: 0.365906\n",
      "\n",
      "Test set: Average loss: 0.1799, Accuracy: 9544/10000 (95.44%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "quad_model = QuadNet(26)\n",
    "optimizer = optim.Adam(quad_model.parameters(), lr=0.0006)\n",
    "args = Parser()\n",
    "args.epochs = 50\n",
    "results['QuadNet'], model, pred_labels = main(\n",
    "    model=quad_model,\n",
    "    args=args,\n",
    "    optimizer=optimizer,\n",
    "    model_type='quad',\n",
    "    task='char',\n",
    "    return_model=True,\n",
    "    return_pred_label=True,\n",
    "    reg_l2=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the model so that we won't have to train it again. _Make sure have the correct path and directories._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../data/models/quad_char_0_01.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_confusion_matrix(pred_labels, task='char')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Setting up the adversarial task\n",
    "\n",
    "We will now use the output of the trained baseline model which is freezed as an input of another model called the `adversarial_model` which will try to predict on another task, namely the family recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are little change compared to the usual test, train and main functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train(args, adversarial_model, model, train_loader, adv_optimizer, epoch):\n",
    "    adversarial_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = model.transform(data)  # <-- This is new\n",
    "        adv_optimizer.zero_grad()\n",
    "        output = adversarial_model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        adv_optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def adversarial_test(args, adversarial_model, model, test_loader):\n",
    "    adversarial_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_labels = None\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = model.transform(data) # <-- This is new\n",
    "            output = adversarial_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            pred_labels_batch = torch.stack((pred, target.view_as(pred))).view(2, args.test_batch_size)\n",
    "            if pred_labels is None:\n",
    "                pred_labels = pred_labels_batch\n",
    "            else:\n",
    "                pred_labels = torch.cat((pred_labels, pred_labels_batch), dim=1).view(2, -1)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), acc))\n",
    "    \n",
    "    return acc, pred_labels.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Adversarial model that we will use. We use the same CNN structure as seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(AdversarialCNN, self).__init__()\n",
    "        self.lin1 = nn.Linear(26, 784)\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.seed = 1\n",
    "        self.test_batch_size = 1000\n",
    "        self.batch_size = 64\n",
    "        self.no_cuda = False\n",
    "        self.save_model = False\n",
    "        self.log_interval = 300\n",
    "        \n",
    "def build_tensor_dataset(data, target):\n",
    "    normed_data = [(d - d.mean()) / d.std() for d in data]\n",
    "    normed_data = torch.stack([torch.Tensor(d).reshape(1, 28, 28) for d in normed_data])\n",
    "    target = torch.LongTensor([i[0] for i in target])\n",
    "    tensor_dataset = utils.TensorDataset(normed_data, target)\n",
    "    return tensor_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base quadratic model is already trained to detect char, which it does not to badly.\n",
    "We compute explicitely the weight sum of the quadratic model parameters to verify that this private model is left unchanged after the collateral learning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_param_norm = quad_model.proj1.weight.norm() + quad_model.diag1.weight.norm()\n",
    "quad_param_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We new try to detect using its output, the family of the original input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = learn.load_data()\n",
    "train_data, train_target_char, train_target_family, test_data, test_target_char, test_target_family = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_phase(model):\n",
    "    args = Parser()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # setting = the family recognition task\n",
    "    train_dataset = build_tensor_dataset(train_data, train_target_family)\n",
    "    test_dataset = build_tensor_dataset(test_data, test_target_family)\n",
    "    adversarial_output_size = 5\n",
    "    \n",
    "    train_loader = utils.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    test_loader = utils.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.test_batch_size, shuffle=True\n",
    "    )\n",
    "    \n",
    "    adversarial_model = AdversarialCNN(output_size=adversarial_output_size)\n",
    "    adversarial_optimizer = optim.SGD(adversarial_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    \n",
    "    test_perfs = []\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        adversarial_train(args, adversarial_model, model, train_loader, adversarial_optimizer, epoch)\n",
    "        acc, pred_labels = adversarial_test(args, adversarial_model, model, test_loader)\n",
    "        test_perfs.append(acc)\n",
    "        \n",
    "    return test_perfs, pred_labels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_perfs, pred_labels = adversarial_phase(quad_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "First, let's verify that the private model weights have not changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert quad_param_norm == quad_model.proj1.weight.norm() + quad_model.diag1.weight.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, one could ask the difference with the setting where we put a CNN (the same) on top of the private network to learn family recognition in Part 3. The major difference is that in Part 3 we could modify the private network weights, while here we couldn't, and moreover the weights were optimised to perform a completely different task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, as this experiment shows, the prediction accuracy of the model is embarrassingly good! This shows that the information contained in the 26 neurons output is far more important than just information about characters. In particular, we can infer quite precisely information about the font of the original data.\n",
    "\n",
    "For the sake of curiosity, we build the confusion matrix of the predictor and compare it to the one with the QuadNet only the the one with the QuadNet + CNN (See Part3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(pred_labels, task='family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
