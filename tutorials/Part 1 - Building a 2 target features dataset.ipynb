{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Encryption - Classification and information leakage\n",
    "\n",
    "Our start point is the work on encrypted classification using Function Encryption of the paper [Reading in the Dark: Classifying Encrypted Digits with Functional Encryption](https://eprint.iacr.org/2018/206), and the associated [GitHub repository](https://github.com/edufoursans/reading-in-the-dark).\n",
    "\n",
    "More specifically, the paper provides a new Functional Encryption scheme for quadratic multi-variate polynomials, which can under some hypothesis be seen as a single hidden layer neural network with a quadratic activation.\n",
    "In the paper, the output corresponds to 1 element per class, so 10 on their example using MNIST, and it is made in clear. The argmax is then taken in clear to determine the class. This approach is standard and was used in [CryptoNets](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/04/CryptonetsTechReport.pdf), [MiniONN](https://eprint.iacr.org/2017/452.pdf), [CryptoDL](https://arxiv.org/pdf/1711.05189.pdf) and other recent papers on Homomorphic Encryption (HE) (like [this paper of 2018](https://arxiv.org/pdf/1807.08459.pdf)).\n",
    "\n",
    "\n",
    "However, there has been very little discussion about the sensitivy of this output, because in the case of MNIST 10 real values are output, which in term of bits of information is huge. Our starting point is therefore: can this output disclose extra information about the initial input or about charasteristics of this input, than just the label we want to classify.\n",
    "\n",
    "To this aim, we have built a specific dataset which is very similar to MNIST, used in the original paper but which is composed of 10 letter characters of 2 differents fonts. Our goal is two-fold:\n",
    "\n",
    " - Evaluate how the output in clear can be leveraged with a public neural network to make better predictions than a simple `argmax` function on the character recognition task.\n",
    " - Analyse to what extent the output in clear of the model can reveal information about the font used, using an \"collateral\" network.\n",
    " \n",
    "But first, let's build this dataset!\n",
    "\n",
    " > Writer identification is not a novel task, several datasets have proposed among which the CDAR 2013 Competition  on  Writer  Identification, ICDAR  2011  Writer  Identification  Contest, and the CVL-Database datasets. However these datasets don't consider characters but complete pages and text extracts which is not suitable for our analysis.\n",
    " \n",
    " > EMNIST however could be an interesting dataset but they don't provide the two labels (value, authors) together. This dataset will be used later to confirm results.\n",
    " \n",
    " > There is also this excellent work @TalwalkarLab to create [FEMNIST](https://github.com/TalwalkarLab/leaf) which can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build the dataset\n",
    "\n",
    "But first, let's build the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from string import ascii_lowercase\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import torch.utils.data as utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done previous work on font distinguishability in a classification task and we've select the two fonts *cursive, Georgia* that are of medium difficulty to distinguish. We will extend our analysis two more couples but this is a fair start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = ['cursive', 'Georgia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here what they look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2))\n",
    "for i, family in enumerate(families):\n",
    "    ax = plt.subplot(1, 5, i+1)\n",
    "    ax.set_title(family)\n",
    "    ax.text(0.3, 0.4, '123', size=50, family=family)\n",
    "\n",
    "    # Rm axes, draw and get the rgba shape of the digits\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Noising process\n",
    "Our dataset is artificial. Hence we need to add some noise to make it more interesting for a classification task.\n",
    "Currently the noise boils down to:\n",
    "\n",
    " - A random but moderate rotation\n",
    " - A random but moderation size variation\n",
    " - A deformation using a Gaussian filter\n",
    "\n",
    "When we are done, we recenter the data (as it is done for MNIST).\n",
    "\n",
    "Let's do it for one character to see how it looks like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions to handle rgb / rgba conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgba_to_rgb(rgba):\n",
    "        if rgba[0] > 0:\n",
    "            return np.ones(3) * (256 - rgba[0])\n",
    "        return rgba[1:]\n",
    "    \n",
    "def convert_to_rgb(data):\n",
    "    return np.apply_along_axis(rgba_to_rgb, 2, data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deformation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dz = np.zeros_like(dx)\n",
    "\n",
    "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]))\n",
    "    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n",
    "\n",
    "    distored_image = map_coordinates(image, indices, order=1, mode='reflect')\n",
    "    return distored_image.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recentering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(data):\n",
    "    # Inverse black and white\n",
    "    wb_data = np.ones(data.shape) * 255 - data\n",
    "    \n",
    "    # normalize\n",
    "    prob_data = wb_data / np.sum(wb_data)\n",
    "    \n",
    "    # marginal distributions\n",
    "    dx = np.sum(prob_data, (1, 2))\n",
    "    dy = np.sum(prob_data, (0, 2))\n",
    "\n",
    "    # expected values\n",
    "    (X, Y, Z) = prob_data.shape\n",
    "    cx = np.sum(dx * np.arange(X))\n",
    "    cy = np.sum(dy * np.arange(Y))\n",
    "    \n",
    "    # Check bounds\n",
    "    assert cx > X/4 and cx < 3 * X/4, f\"ERROR: {cx} > {X/4} and {cx} < {3 * X/4}\"\n",
    "    assert cy > Y/4 and cy < 3 * Y/4, f\"ERROR: {cy} > {Y/4} and {cy} < {3 * Y/4}\"\n",
    "    \n",
    "    # print('Center', cx, cy)\n",
    "    \n",
    "    x_min = int(round(cx - X/4))\n",
    "    x_max = int(round(cx + X/4))\n",
    "    y_min = int(round(cy - Y/4))\n",
    "    y_max = int(round(cy + Y/4))\n",
    "    \n",
    "    return data[x_min:x_max, y_min:y_max, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One first example!\n",
    "Ok let's how it looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a rotation angle\n",
    "rotation = np.random.normal(0, 10) # 10 degrees (out of 180)\n",
    "# Sample a text size +/- 5% in std\n",
    "size = 100 + np.random.normal(0, 5) \n",
    "# Sample a family\n",
    "family_idx = np.random.randint(len(families))\n",
    "family = families[family_idx]\n",
    "# Sample a char\n",
    "digit = np.random.randint(10)\n",
    "letter = str(digit)\n",
    "\n",
    "# Show rotated letter\n",
    "fig = plt.figure(figsize=(2,2), dpi=100)\n",
    "fig.text(0.4, 0.4, letter, size=size, rotation=rotation, family=family)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Load the letter in smaller size\n",
    "fig = plt.figure(figsize=(2,2), dpi=100)\n",
    "fig.text(0.4, 0.4, letter, size=50, rotation=rotation, family=family)\n",
    "\n",
    "# Rm axes, draw and get the rgba shape of the letter\n",
    "plt.axis('off')\n",
    "fig.canvas.draw()\n",
    "data = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "plt.close(fig)\n",
    "# Convert to rgb\n",
    "data = convert_to_rgb(data)\n",
    "\n",
    "# Re-center the data\n",
    "data = center(data)\n",
    "\n",
    "plt.show()\n",
    "plt.axis('off')\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "# Apply an elastic deformation\n",
    "data = elastic_transform(data, alpha=991, sigma=9)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "\n",
    "# Free memory space\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the images here are provided with high resolution, they will then be converted to 28x28 pixels and will becomes way harder to analyse.\n",
    "\n",
    "Feel free to change the rotation std, the text size std, and the `alpha` and `sigma` parameters in the deformation. You'll see that the task can be made incredibly harder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the dataset\n",
    "Ok now we are equipped to build a complete dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a variance parameter for rotation and one for text size\n",
    "std_rotation = 10\n",
    "std_size = 2.5 # 5% of reference size, here 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def build_dataset(dataset_size, verbose=False):\n",
    "    dataset_data = []\n",
    "    dataset_target_char = []\n",
    "    dataset_target_family = []\n",
    "    for i in range(dataset_size):\n",
    "        if i % int(dataset_size/100) == 0:\n",
    "            print(round(i / dataset_size * 100), '%')\n",
    "        rotation = np.random.normal(0, std_rotation)\n",
    "        size = 50 + np.random.normal(0, std_size) \n",
    "        family_idx = np.random.randint(len(families))\n",
    "        family = families[family_idx]\n",
    "        digit = np.random.randint(10)\n",
    "        letter = str(digit)\n",
    "\n",
    "        fig = plt.figure(figsize=(2,2), dpi=28)\n",
    "        fig.text(0.4, 0.4, letter, size=size, rotation=rotation, family=family)\n",
    "\n",
    "        # Rm axes, draw and get the rgba shape of the letter\n",
    "        plt.axis('off')\n",
    "        fig.canvas.draw()\n",
    "        data = np.frombuffer(fig.canvas.tostring_argb(), dtype=np.uint8)\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "\n",
    "        # Convert to rgb\n",
    "        data = convert_to_rgb(data)\n",
    "\n",
    "        # Center the data\n",
    "        data = center(data)\n",
    "\n",
    "        if verbose:\n",
    "            plt.show()\n",
    "            plt.axis('off')\n",
    "            plt.imshow(data)\n",
    "            plt.show()\n",
    "            \n",
    "        # Apply an elastic deformation\n",
    "        data = elastic_transform(data, alpha=991, sigma=9)\n",
    "\n",
    "        if verbose:\n",
    "            plt.axis('off')\n",
    "            plt.imshow(data)\n",
    "            plt.show()\n",
    "\n",
    "        # Free memory space\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Append data to the datasets\n",
    "        target_char = np.array([digit])\n",
    "        target_family = np.array([family_idx])\n",
    "        dataset_data.append(data[:,:,0])\n",
    "        dataset_target_char.append(target_char)\n",
    "        dataset_target_family.append(target_family)\n",
    "        \n",
    "    return dataset_data, dataset_target_char, dataset_target_family\n",
    "\n",
    "\n",
    "def save_dataset(info, data, target_char, target_family):\n",
    "    with open('dataset/character_dataset_{}.pkl'.format(info), 'wb') as output:\n",
    "        dataset = data, target_char, target_family\n",
    "        pickle.dump(dataset, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# We build 10000 per 10000 for memory purposes\n",
    "# WARNING: This step takes time, and you may need to restart you notebook for memory purposes.\n",
    "for i in range(6):\n",
    "    print(f\"Train{i}\")\n",
    "    train_data, train_target_char, train_target_family = build_dataset(10000)\n",
    "    save_dataset(f\"train{i}\", train_data, train_target_char, train_target_family)\n",
    "    \n",
    "print(\"Test\")\n",
    "test_data, test_target_char, test_target_family = build_dataset(10000)\n",
    "save_dataset(\"test\", test_data, test_target_char, test_target_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we now have our dataset! Let's use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "Here is an utility function to draw some samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to load packages from parent\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "import random, math\n",
    "import learn\n",
    "data = learn.load_data()\n",
    "train_data, train_target_char, train_target_family, test_data, test_target_char, test_target_family = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(data, target_char, target_family, n_examples=20):\n",
    "    n_rows = math.ceil(n_examples / 5)\n",
    "    plt.figure(figsize=(14, 3*n_rows))\n",
    "    for i in range(n_examples):\n",
    "        ax = plt.subplot(n_rows, 5, i+1)\n",
    "        idx = random.randint(0, len(data))\n",
    "        image = data[idx]\n",
    "        letter = str([target_char[idx][0]])\n",
    "        family = families[target_family[idx][0]]\n",
    "        ax.set_title(f\"{letter} ({family})\")\n",
    "        plt.axis('off')\n",
    "        ax.imshow(image, cmap='gist_gray')\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "show_examples(test_data, test_target_char, test_target_family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now much harder to distinguish the fonts, isn't it?!\n",
    "\n",
    "Ok let's use this dataset to start out work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### If you like it, star it!\n",
    "\n",
    "The easiest way to show support is just by starring the Repos! This helps raise awareness on this topics and is a precious feedback for the repo maintainers!\n",
    "\n",
    "- [Star the Repo](https://github.com/LaRiffle/collateral-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
